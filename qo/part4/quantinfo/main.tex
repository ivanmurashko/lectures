%% -*- coding:utf-8 -*- 
\chapter{Введение в квантовую теорию информации}
\label{chQuantInfo}

Классическая теория информации была развита Клодом Шеноном
\cite{bShenon} в применении к задачам теории связи и в данный момент
находит все большее 
применение. В этом разделе мы рассмотрим основные понятия и результаты
теории информации, в том числе в применении ее к квантовым явлениям.

\section{Информация и энтропия}

Само понятие информации тесно связано с мерой неопределенности
системы, т. е. тем, что в статистической физике называется
энтропией. Чем более неупорядочена система, тем меньше информации нам
о ней известно. Приобретение информации уменьшает меру
неопределенности. Соответственно, чем более неупорядочена была исходная
система, тем больше информации мы о ней приобретаем в результате исхода
некоторого опыта. 
%
% В качестве примера рассмотрим информацию о
% погоде. Допустим нам интересно следующее событие - будет ли в
% определенный день снег или нет. Если этот день - 1 июля то мы имеем
% вполне определенную (упорядоченную) систему и если нам скажут 2 июля
% что 1-ого числа снега не 
% было это нам не даст никакой информации потому что результат был и так
% предсказуем. Вместе с тем тот же самый опыт проведенный скажем 1
% февраля уже не является определенным и дает какую-то информацию.
%
Таким образом в качестве меры информации можно взять сложность
(неопределенность) системы. Следуя в дальнейшем Шенону мы будем эту
меру называть энтропией, при этом мы не будем делать различия между
``мерой количества информации'' и ``энтропией'', т. е. будем считать эти
понятия синонимами.

\subsection{Энтропия в классической теории информации}

Определение понятия энтропии мы начнем с опыта с равновероятными
исходами. Допустим у нас имеется некоторое событие $\mathcal{A}$,
которое имеет $a$ равновероятных исходов. В качестве энтропии (меры
неопределенности) разумно использовать некоторую положительную функцию,
которая будет зависеть от числа исходов $a$ и будет возрастать с ростом $a$:
\begin{equation}
H_{\mathcal{A}} = f\left(\mathcal{A}\right) = f\left(a\right).
\nonumber
\end{equation}
При этом разумно предположить, что $f\left(1\right) = 0$, т. к. результат опыта с
одним возможным значением полностью определен.

Расмотрим теперь еще один опыт $\mathcal{B}$ с числом исходов
$b$. Очевидно, что для него справедливо соотношение
\[
H_{\mathcal{B}} = f\left(b\right).
\]
Если же рассмотреть событие $\mathcal{A}\mathcal{B}$,
которое состоит в одновременном выполнении событий $\mathcal{A}$ и
$\mathcal{B}$, то число равновероятных исходов такого
события есть величина $a\cdot b$, т. е. можно записать
\[
H_{\mathcal{A}\mathcal{B}} = f\left(ab\right).
\]
С другой стороны хотелось бы выразить $H_{\mathcal{A}\mathcal{B}}$
через $H_{\mathcal{A}}$ и $H_{\mathcal{B}}$. Для этого вспомним
свойство роста энтропии с ростом сложности системы, т. е. справедливо
следующее неравенство
\begin{equation}
H_{\mathcal{A}\mathcal{B}} \ge H_{\mathcal{A}}, H_{\mathcal{B}}.
\label{eqEntropyComp}
\end{equation}
Если для искомой связи использовать только 4 простейшие арифметические
операции, то только сложение и умножение будут удовлетворять свойству
\eqref{eqEntropyComp}. При этом если рассмотреть ситуацию, когда число
исходов события $\mathcal{B}$ равно 1, то окажется, что для искомой
связи может быть использована только операция сложения:
\begin{equation}
H_{\mathcal{A}\mathcal{B}} = H_{\mathcal{A}} + H_{\mathcal{B}},
\nonumber
%\label{eqEntropyComp1}
\end{equation}
откуда
\begin{equation}
f\left(ab\right) = f\left(a\right) + f\left(b\right).
\nonumber
%\label{eqEntropyProperty1}
\end{equation}
Соберем теперь вместе те сведения, которые нам известны про искомую
функцию $f$
\begin{eqnarray}
f\left(1\right) = 0,
\nonumber \\
f\left(a\right) > f\left(b\right), \mbox{ если } a > b,
\nonumber \\
f\left(a b\right) = f\left(a\right) + f\left(b\right).
\label{eqEntropyProperty}
\end{eqnarray}
Существует только одна непрерывная функция, которая удовлетворяет
требованиям \eqref{eqEntropyProperty} (доказательство
см. \cite{bYaglom}):
\begin{equation}
f\left(a\right) = K\,log\,a.
\label{eqEntropyDef1}
\end{equation}
Коэффициент $K$ в \eqref{eqEntropyDef1} определяет показатель логарифма и величину единицы
измерения информации. Например если мы используем в качестве единицы
измерения энтропию опыта с двумя равновероятными исходами (например
выпадения ``орла'' или ``решки'' при бросании монеты), то в формуле
\eqref{eqEntropyDef1} используется логарифм по основанию 2:
\begin{equation}
f\left(a\right) = log_2\,a,
\label{eqEntropyDefBit}
\end{equation}
а соответствующая единица измерения информации называется {\bf бит}\footnote{от
англ. {\bf{bi}}nary digi{\bf{t}} - двоичный разряд}. Выбор этой
единицы при измерении количества информации
объясняется тем, что часто информация кодируется с помощью объектов,
которые имеют только два стабильных состояния (например, превышает
сигнал некоторый уровень или нет). Наряду с логарифмом по основанию 2 в \eqref{eqEntropyDef1}
часто используется натуральный логарифм:
\begin{equation}
f\left(a\right) = ln\,a,
\label{eqEntropyDefNat}
\end{equation}
соответствующая единица измерения информации называется {\bf нат}. В
дальнейшем мы будем использовать определение энтропии в форме
\eqref{eqEntropyDef1} при этом основание логарифма будет приниматься
равным 2.

Выражение \eqref{eqEntropyDef1} получено нами в предположении
равновероятных исходов. Как известно, вероятность события
$\mathcal{A}$ определяется как
\[
p_{\mathcal{A}} = \frac{1}{a},
\]
откуда энтропия $H_{\mathcal{A}}$ может быть записана как
\begin{equation}
H_{\mathcal{A}} = -log \, p_{\mathcal{A}}.
\label{eqEntropy11}
\end{equation}

Рассмотрим теперь случай, когда событие $\mathcal{A}$ имеет не
равновероятные исходы. В этом случае каждому исходу $\xi$ события
$\mathcal{A}$ следует сопоставить вероятность этого исхода
$p_{\xi}$. Для определения меры количества информации, которое мы
приобретем при реализации исхода $\xi$, мы будем попрежнему
использовать выражение \eqref{eqEntropy11}:
\begin{equation}
H_{\xi} = -log \, p_{\xi}.
\label{eqEntropy12}
\end{equation}
% Поясним это выражение на простейшем примере. Допустим что вероятность
% того, что некоторый студент сдаст зачет равна $0.9$. Соотвественно
% вероятность того, что зачет не будет сдан - $0.1$. Если результат сдачи
% зачета будет положительным, то количество приобретенной информации
% будет $\approx 0.1$ нат, что меньше чем в случае несдачи зачета
% $\approx 2.3$ нат. Действительно в первом случае (в отличии от
% второго) мы вправе говорить о том, что почти не получили никакой информации
% потому как и так были уверены что этот студент сдаст зачет.

Величина $H_{\xi}$ в \eqref{eqEntropy12} является характеристикой
только одного конкретного исхода события $\xi$ и, соответственно, не
может быть использована в качестве меры неопределенности всего события
$\mathcal{A}$. В качестве такой меры разумно использовать усредненное
значение энтропии:
\begin{equation}
H_{class} = H_{\mathcal{A}} = \sum_{\xi} - p_{\xi} log \, p_{\xi}.
\label{eqEntropyClass}
\end{equation}
В выражении \eqref{eqEntropyClass} принимается что
\begin{equation}
0 \cdot log 0 = 0.
\label{eqEntropyClassAdd}
\end{equation}

Несмотря на кажущуюся условность ввода \eqref{eqEntropyClass}, это
выражение действительно описывает количество информации, в частности,
живые организмы обрабатывают информацию со скоростью которая
пропорциональна \eqref{eqEntropyClass}. В \cite{bYaglom} 
%\footnote{не оригинальный источник FIX ME!!!}
описаны эксперименты по скорости реакции человека
на внешний раздражитель. В качестве раздражителя выступала одна из нескольких
лампочек, каждая из которых загоралась с разной частотой. Перед
экспериментом экзаменуемым давалось время на тренировку, после которой оказывалась,
что скорость реакции определяется именно выражением \eqref{eqEntropyClass}.

Энтропия \eqref{eqEntropyClass} обладает следующим свойством:
\begin{equation}
H_{min} \le H_{\mathcal{A}} \le H_{max},
\label{eqEntropyClassProperty}
\end{equation}
при этом $H_{min} = 0$ и достигается в случае, когда вероятность одного
из исходов события $\mathcal{A}$ равна 1, а остальных 0 (минимальная
неопределенность системы). $H_{max} = log a$ и достигается, когда все
исходы равновероятны (максимальная неопределенность системы).

\subsection{Энтропия в квантовых системах}

Аналогом распределения вероятностей $\left\{p_{\xi}\right\}$ в
квантовом случае служит матрица плотности \index{Матрица плотности}
$\hat{\rho}$. При этом в
качестве усреднения используется операция взятия следа в некотором
базисе. То есть выражению \eqref{eqEntropyClass} в квантовых системах
соответствует:
\begin{equation}
H_{quant} = - Sp \left(\hat{\rho} \, log \, \hat{\rho}\right).
\label{eqEntropyQuant}
\end{equation}

При анализе соотношения \eqref{eqEntropyQuant} возникают следующие
вопросы:
\begin{itemize}
\item Как считать энтропию \eqref{eqEntropyQuant}
\item В каких случаях энтропия будет максимальна и в каких минимальна,
  т. е. каким будет квантовый аналог классического выражения
  \ref{eqEntropyClassProperty}. 
\end{itemize}

Для ответа на первый вопрос вспомним что операция взятия следа не
зависит от представления (см. \autoref{AddDiracTrace}), поэтому
достаточно рассмотреть матрицу плотности в том базисе, в котором она имеет
диагональное представление. Если обозначить через $\rho_{nn}$
диагональный элемент матрицы плотности, то с помощью \eqref{eqAddDiracFL}
можно будет переписать \eqref{eqEntropyQuant} следующим образом
\begin{equation}
H_{quant} = - Sp \left(\hat{\rho} \, log \, \hat{\rho}\right) = 
- \sum_n \rho_{nn} \, log \, \rho_{nn}.
\label{eqEntropyQuant1}
\end{equation}

Начнем анализ выражения \eqref{eqEntropyQuant1} с рассмотрения случая чистых состояний.
Если взять произвольное чистое состояние $\left|\psi\right>$,
записанное в некотором базисе $\left\{\left|n\right>\right\}$:
\[
\left|\psi\right> = \sum_n c_n \left|n\right>,
\]
то всегда можно найти такой базис $\left\{\left|m\right>\right\}$, в
котором $\left|\psi\right>$ будет одним из базисных состояний, например первым: 
%\footnote{FIX ME!!! добавь ссылку или раскрыть это утверждение}
\[
\left|\psi\right> = \sum_m c_m \left|m\right> = 
\left.\left|m\right>\right|_{m = 1},
\]
таким образом, в диагональной матрице плотности останется только один
не нулевой член $\rho_{11} = 1$, и энтропия будет равна
\begin{equation}
H_{pure} = 0,
\label{eqQIEntropyPure}
\end{equation}
при этом мы использовали выражение \eqref{eqEntropyClassAdd}. Это
выражение говорит нам о том, что чистое состояние является полностью
определенным. 

Если рассматривать смешанное состояние, то матрица плотности 
\index{Матрица плотности}
будет
диагональной, при этом сами диагональные элементы будут равны
вероятностям нахождения системы в соответствующем состоянии:
\[
\rho_{nn} = p_n.
\]
Если теперь воспользоваться \eqref{eqEntropyQuant1}, то можно получить,
что выражение для энтропии смешанного состояния совпадает с
классическим выражением энтропии \eqref{eqEntropyClass}:
\begin{equation}
H_{misc} = - \sum_n p_n \, log \, p_n = H_{class},
\label{eqQIEntropyMisc}
\end{equation}
которое, как уже указывалось ранее, максимально при равномерном распределении
вероятностей и минимально при вырожденном (когда один из исходов имеет
вероятность 1, а остальные 0).

%% Посмотрев на выражения \eqref{eqQIEntropyPure} и
%% \eqref{eqQIEntropyMisc} можно заключить, что процесс декогеренции
%% \footnote{Процесс разрушения чистового состояния - когда чистое состояние из-за
%% взаимодействия с внешним окружением превращается в смешанное} можно
%% рассматривать как процесс потери информации о квантовой системе.

Наряду с выражениями \eqref{eqEntropyQuant1} и \eqref{eqQIEntropyMisc},
в которых единицей измерения количества информации является нат удобно
использовать логарифм с основанием два - в этом случае единица
измерения будет называться кубит ({\bf q}uantum {\bf bit}).

\section{Передача информации. Канал связи.}

Такие общие характеристики информации как энтропия не учитывают самого
конкретного содержания информации, которое также может быть важно, и
соответственно, могут быть применены не всегда. 
Вместе с тем существуют ситуации, когда конкретное значение не важно,
а имеет значение именно количество информации. В качестве 
примера таких систем можно рассмотреть системы передачи информации.

\input ./part4/quantinfo/figchanel.tex

% Например допустим что у
% нас имеются два события которые характеризуются исходами которые
% представлены в следующих таблицах:
% \begin{longtable}{|c|c|}
% \hline
% Значение & Вероятность \\ \hline
% 100  & 0.3 \\
% 100.1  & 0.3 \\
% 50  & 0.4 \\ \hline
% \end{longtable}
% \begin{longtable}{|c|c|}
% \hline
% Значение & Вероятность \\ \hline
% 150  & 0.3 \\
% 100  & 0.4 \\
% 50  & 0.3 \\ \hline
% \end{longtable}
% Очевидно энтропия в обеих случаях будет одинаковой, вместе с тем стоит
% отметить что первые два результата в первой таблице могут быть вызваны
% погрешностью измерительного прибора и скорее всего должны
% рассматриваться как один.

\subsection{Классический канал связи}

Типичная система передачи информации изображена на
\autoref{figQITransfer}. Как видно из этого рисунка, прежде чем
информация будет передана, она должна быть закодирована,
т. е. представлена в тех состояниях, которые могут быть переданы через
канал. На выходе канала информация должна быть раскодирована,
т. е. преобразована в ту форму, которая использовалась на входе. 

Наиболее удобным представляется кодирование информации классическими
объектами, которые имеют всего два стабильных состояния (например
превышение или нет мощностью сигнала некоторого порога). Очевидно, что
классические состояния выбираются таким образом, чтобы их можно было
легко детектировать. 

\subsection{Квантовый канал связи}

Использование для передачи информации квантовых объектов значительно
расширяет возможности канала связи, потому как у квантовой системы
имеется гораздо больший выбор состояний, которые могут быть использованы для
передачи информации.

Вместе с тем и сам канал связи в квантовом случае характеризуется
некоторыми особыми свойствами. В качестве примера можно рассмотреть
передачу информации, которая кодируется с помощью чистых квантовых
состояний. Как уже указывалось ранее (см. \ref{pPart3EntangleNoClone}),
произвольное квантовое состояние не может быть копировано, откуда
вытекает, что в такой системе, в отличии от классического канала связи,
невозможна не только подмена информации, но даже ее
несанкционированное чтение. Это имеет особое значение для защиты
информации. 

\subsection{Квантовая плотная кодировка}
\label{subsecPart3QuantInfoBigCoding}
В качестве примера квантового канала связи рассмотрим протокол
квантовой плотной кодировки, предложенный Беннетом и Виснером
\cite{bBennettWiesner}, \cite{bDenseCodeExp}. В этом протоколе
используется перепутанное по поляризации двухфотонное состояние,
рассмотренное нами ранее  (см. гл. \ref{chEntangl}).  

Каждый из фотонов по отдельности имеет два ортогональных состояния 
$\left|x\right>_{1,2}$ и $\left|y\right>_{1,2}$. Таким образом с
классических позиций с помощью этих двух фотонов можно закодировать
четыре бита информации, для этого можно использовать следующие
состояния:
$\left|x\right>_1 \left|x\right>_2$,
$\left|x\right>_1 \left|y\right>_2$,
$\left|y\right>_1 \left|x\right>_2$,
и 
$\left|y\right>_1 \left|y\right>_2$.

\input ./part4/quantinfo/figdensecode.tex

В протоколе квантовой плотной кодировки тоже можно закодировать 4
бита информации, но при этом оперируя {\it только одним} из двух 
фотонов. Схема этого протокола изображена на
\autoref{figPart3QuantInfoDenseCode}. Источник $S$ производит пару
перепутанных фотонов в некотором белловском состоянии, например
$\left|\Psi^{+}\right>_{12}$.  Первый фотон из этой пары отсылается Алисе,
а второй Бобу. Изменяя свойства своего (второго) фотона, Боб
преобразует исходное состояние к одному из 4 белловских состояний \eqref{eqEntangBellBase}:
\begin{eqnarray}
  \left|\psi^{+}\right>_{12} = 
  \frac{1}{\sqrt{2}}\left(
  \left|x\right>_1\left|y\right>_2 + 
  \left|y\right>_1\left|x\right>_2
  \right),
  \nonumber \\
  \left|\psi^{-}\right>_{12} = 
  \frac{1}{\sqrt{2}}\left(
  \left|x\right>_1\left|y\right>_2 - 
  \left|y\right>_1\left|x\right>_2
  \right),
  \nonumber \\
  \left|\phi^{+}\right>_{12} = 
  \frac{1}{\sqrt{2}}\left(
  \left|x\right>_1\left|x\right>_2 + 
  \left|y\right>_1\left|y\right>_2
  \right),
  \nonumber \\
  \left|\phi^{-}\right>_{12} = 
  \frac{1}{\sqrt{2}}\left(
  \left|x\right>_1\left|x\right>_2 - 
  \left|y\right>_1\left|y\right>_2
  \right).
  \label{eqQuantInfoBellBase}
\end{eqnarray}

В случае состояния $\left|\Psi^{+}\right>_{12}$ никаких
дополнительных действий не требуется, т. к. оно является исходным: 
\begin{equation}
\left|y\right>_2 \rightarrow \left|y\right>_2, \, 
\left|x\right>_2 \rightarrow \left|x\right>_2.
\label{eqQuantInfoDense1}
\end{equation}
Для получения состояния $\left|\psi^{-}\right>_{12}$ Боб должен
произвести зависимый от поляризации фазовый сдвиг:
\begin{equation}
\left|y\right>_2 \rightarrow \left|y\right>_2, \, 
\left|x\right>_2 \rightarrow e^{i \pi}\left|x\right>_2.
\label{eqQuantInfoDense2}
\end{equation}
В случае состояния $\left|\phi^{+}\right>_{12}$ Боб производит
изменение поляризации на ортогональное
\begin{equation}
\left|y\right>_2 \rightarrow \left|x\right>_2, \, 
\left|x\right>_2 \rightarrow \left|y\right>_2.
\label{eqQuantInfoDense3}
\end{equation}
И, наконец, для состояния $\left|\phi^{-}\right>_{12}$ Боб производит
одновременно и смену поляризации и зависимый от поляризации фазовый
сдвиг:
\begin{equation}
\left|y\right>_2 \rightarrow \left|x\right>_2, \, 
\left|x\right>_2 \rightarrow e^{i \pi}\left|y\right>_2.
\label{eqQuantInfoDense4}
\end{equation}

После того как Боб произвел одно из преобразований
\eqref{eqQuantInfoDense1} - \ref{eqQuantInfoDense4} он отсылает свою
частицу Алисе. Алиса в свою очередь измеряет полученное белловское
состояние \eqref{eqQuantInfoBellBase}, например с помощью детектора
описанного в \ref{pPart3EntangleBellReg}, определяет какое из четырех
действий было совершено Бобом. Таким образом Боб может передать Алисе
4 бита информации.

\input ./part4/quantinfo/code.tex


\section{Упражнения}
\begin{enumerate}
\item Доказать квантовое выражение для энтропии  \eqref{eqEntropyQuant1}.  
\end{enumerate}


%% \begin{thebibliography}{99}
%% \bibitem{bShenon} C. E. Shannon, A Mathematical Theory of
%%   Communication, Bell System Technical Journal, 27, pp. 379-423, 
%%   623-656, July, October, 1948
%% \bibitem{bYaglom} А. М. Яглом, И. М. Яглом,
%%   Вероятность и информация, М. Наука, 1973,
%%   512 с.
%% \bibitem{bStratonovich}  
%%   Р. Л. Стратонович,
%%   Теория информации, М. Советское радио, 1975,
%%   424 с.
%% \bibitem{bPhisQuantInfo} Физика квантовой информации, под
%%   ред. А. Цайлингера. М. Постмаркет 2002
%% \bibitem{bBennettWiesner} C. Bennett, S. Wiesner, Communication via one- and
%%   two-particle operators on Einstein-Podolsky-Rosen states,
%%   Phys. Rev. Lett. 69 (1992) pp. 2881-2884. 
%% \bibitem{bDenseCodeExp} K. Mattle, H. Weinfurter, P. G. Kwiat,
%%   and A. Zeilinger, Dense Coding in Experimental Quantum
%%   Communication, Phys. Rev. Lett. 76, 4656 (1996). 
%% \bibitem{bShenonCrypto} C. E. Shannon, Communication Theory of Secrecy
%%   Systems. Bell System Technical Journal 28 (4), pp.  656-715. 
%% \end{thebibliography} 
