%% -*- coding:utf-8 -*- 
\section{Кодирование информации}

\subsection{Теорема кодирования Шенона}
Допустим у нас имеется сообщение, которое может состоять из
некоторого ограниченного набора символов (алфавита). Предположим, что
в нашем алфавите $k$ символов:
\begin{equation}
\left\{
a_1, \dots a_k.
\right\}
\label{eqShenonAlphabetClass}
\end{equation}
Каждый символ появляется с некоторой известной вероятностью
$p_k = p\left(a_k\right)$. Далее рассмотрим сообщение, состоящие из $n
\gg 1$ символов. Нас интересует вопрос: можно ли сжать это сообщение,
т. е. можно ли закодировать это сообщение таким образом, чтобы
результат содержал меньше чем $n$ символов и при этом была бы
возможность получить исходное сообщение без потери информации.

Ответ был дан Шеноном, и он гласит, что максимальное сжатое сообщение
будет состоять из $n H$ бит, где $H = - \sum_k p_k log p_k$ -
количество информации, которое характеризует наш алфавит. Данный
результат известен как теорема Шенона для канала без шума.

В качестве примера удобно использовать двоичный алфавит, при этом 0
будет встречаться с вероятностью $p$, а 1 с вероятностью $1 -
p$. Таким образом, если у нас имеется сообщение длиной $n$ бит - то
имеется возможность сжать его до $n H$ бит. При этом очевидно что 
\begin{equation}
n \ge n H > 0.
\label{eqShenonAlphabetClassBit}
\end{equation}
При этом знак равенства в \eqref{eqShenonAlphabetClassBit} будет иметь
место только при равновероятном распределении вероятности $p =
\frac{1}{2}$. Действительно в этом случае 
\[
n H = n \left(- \frac{1}{2} log \frac{1}{2} - \frac{1}{2} log
  \frac{1}{2}\right) = n.
\]

\subsection{Квантовая теорема кодирования}
Как будет выглядеть квантовый аналог теоремы Шенона для канала без
шума? Для это прежде всего рассмотрим квантовый аналог алфавита
\eqref{eqShenonAlphabetClass}. Квантовое сообщение кодируется
некоторым набором состояний 
\begin{equation}
\left\{
\ket{a_1}, \dotsc \ket{a_k}.
\right\}
\label{eqSchumacherAlphabet}
\end{equation}
Каждое из состояний появляется с некоторой вероятностью $p_k =
p\left(\ket{a_k}\right)$. Таким образом мы можем записать
матрицу плотности для каждого символа нашего сообщения:
\begin{equation}
\hat{\rho} = \sum_k p_k \ket{a_k}\bra{a_k}.
\label{eqQuantCodeMatrix}
\end{equation}
Сообщение, состоящее из $n$ символов, имеет следующую матрицу плотности
\begin{equation}
\hat{\rho}^n = \hat{\rho}\otimes \dots \otimes \hat{\rho}.
\nonumber
\end{equation}
Можно ли сжать это сообщение и если да, то до какого предела? Ответ
дает квантовая теорема кодирования для канала без шума (теорема
Шумахера). Предел достигается при использовании 
\begin{equation}
n H = - n Sp \left(\hat{\rho} log \hat{\rho} \right),
\nonumber
\end{equation}

В качестве примера расмотрим кодировку с помощью поляризационных
состояний фотона. Допустим, что наш алфавит состоит из двух
состояний:
\begin{enumerate}
\item $\ket{x}$ - фотон поляризован по оси $x$ 
\item $\ket{y}$ - фотон поляризован по оси $y$
\end{enumerate}
При это предполагаем, что состояние $\ket{x}$ встречается с
вероятностью $p_{\ket{x}} = p$, а состояние $\ket{y}$ - с
вероятностью $p_{\ket{y}} = 1 - p_{\ket{x}} = 1 - p$.
Таким образом матрица плотности \index{Матрица плотности}
\eqref{eqQuantCodeMatrix} будет
иметь вид
\begin{equation}
\hat{\rho} = p \ket{x}\bra{x} + \left(1 - p\right)
\ket{y}\bra{y}.
\label{eqQuantCodeMatrixQBIT}
\end{equation}
В этом случае удобно
использовать в качестве единицы измерения информации кубит и, соответственно,
если исходное сообщение кодируется $n$ фотонами, то его можно
сжать до 
\(
n H = - n Sp \left(\hat{\rho} log \hat{\rho} \right)
\)
кубит, т. е. можно использовать меньше чем $n$ фотонов \index{фотон} для передачи
этого сообщения. Главное отличие квантового случая от классического в
том, что возможно сжатие информации даже в том случае, если $p =
\frac{1}{2}$. 




